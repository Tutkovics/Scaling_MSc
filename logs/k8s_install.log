[local machine]
mkdir kubernetes_install 
git clone git@github.com:kubernetes-sigs/kubespray.git
python3 -V
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
sudo python3 get-pip.py
pip3 -V
pip3 install virtualenv
virtualenv -p /usr/bin/python3 venv
source venv/bin/activate

sudo pip3 install -r requirements.txt
cp -r inventory/sample inventory/dipterv
declare -a IPS=(10.151.103.1 10.151.103.2 10.151.103.3)
python3 -m pip install ruamel.yaml
CONFIG_FILE=inventory/dipterv/hosts.yaml python3 contrib/inventory_builder/inventory.py ${IPS[@]}


vim inventory/dipterv/group_vars/k8s-cluster/k8s-cluster.yml
kube_network_plugin: cilium
## Automatically renew K8S control plane certificates on first Monday of each month
auto_renew_certificates: true
cluster_name: dipterv.cluster.local


vim inventory/dipterv/group_vars/k8s-cluster/addons.yml
helm_enabled: true
# Metrics Server deployment
metrics_server_enabled: true
metrics_server_kubelet_insecure_tls: true
# Cert manager deployment
cert_manager_enabled: true
cert_manager_namespace: "cert-manager"

vim inventory/dipterv/group_vars/all/all.yml
use_localhost_as_kubeapi_loadbalancer: true

vim roles/network_plugin/cilium/defaults/main.yml 

vim inventory/dipterv/inventory.ini
[bastion] 
bastion ansible_host=152.66.211.2
(Nem így telepítettem) 

[all hosts]
# echo 1 > /proc/sys/net/ipv4/ip_forward 
cat /proc/sys/net/ipv4/ip_forward

Végül a /inventory/dipterv és alatta lévő konfigfájlokat a dipterv1 node-on hoztam létre és futtattam az ansible playbook-ot. Ezzel a set-uppal működött rendben. 
Jumper Server-rel sehogy nem akart összejönni, pedig már mindenhova minden kulcsot és konfigot beállítottam.
```
ansible-playbook -i inventory/dipterv/hosts.yaml  --become --become-user=root cluster.yml
```
Install time: 1 óra 54 perc --> sikertelene modeprobe nem ment

New try after: modprobe nf_conntrack

Change Cilium to Calico

Try again (change kubectl_localhost, kubeconfig_localhost)

Delete kubespray install
ansible-playbook -i inventory/dipterv/hosts.yaml reset.yml --become --become-user=root

## Kubeadm install ## 
tutkovics@node1:~/Downloads$ sudo kubeadm init --config=kubeadm-config.yaml | tee kubeadm-init.out                                                                   
W0406 21:02:38.607005    4522 common.go:77] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta1". Please use 'kubeadm config migrate --old-co
nfig old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.                                                              
[init] Using Kubernetes version: v1.20.5                                                                                                                             
[preflight] Running pre-flight checks                                                                                                                                
        [WARNING IsDockerSystemdCheck]: detected "cgroupfs" as the Docker cgroup driver. The recommended driver is "systemd". Please follow the guide at https://kube
rnetes.io/docs/setup/cri/                                                                                                                                            
        [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 20.10.5. Latest validated version: 19.03                         
        [WARNING SystemVerification]: missing optional cgroups: hugetlb                                                                                              
        [WARNING Hostname]: hostname "node1" could not be reached                                                                                                    
        [WARNING Hostname]: hostname "node1": lookup node1 on 152.66.208.1:53: no such host                                                                          
[preflight] Pulling images required for setting up a Kubernetes cluster  
...
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities                                                                               
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.dipterv1:6443 --token l2eh71.tun5gdbbny7htf7x \
    --discovery-token-ca-cert-hash sha256:05195e22f0f3a6925da8a0454c1b66cda0256162f7076e8890765407a83ddd13 \                                                        
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:                                                                              

kubeadm join apiserver.dipterv1:6443 --token l2eh71.tun5gdbbny7htf7x \
    --discovery-token-ca-cert-hash sha256:05195e22f0f3a6925da8a0454c1b66cda0256162f7076e8890765407a83ddd13


Rossz (külső ip) címet hirdetett a master + ki is fogyott a  memóriából
kubeadm reset

és újra az egész


Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.dipterv1:6443 --token 0d9jht.08yw6uq9y1iix9d4 \
    --discovery-token-ca-cert-hash sha256:f433e44c6c1099f43110f5    b7214af466dd85e0f7c06ea5cf901ccbab2fc6db58 \                                                                    
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join apiserver.dipterv1:6443 --token 0d9jht.08yw6uq9y1iix9d4 \
    --discovery-token-ca-cert-hash sha256:f433e44c6c1099f43110f5b7214af466dd85e0f7c06ea5cf901ccbab2fc6db58   

utkovics@node1:~/Downloads$ kubectl get nodes 
NAME    STATUS   ROLES                  AGE     VERSION
node1   Ready    control-plane,master   12m     v1.20.5
node2   Ready    <none>                 6m1s    v1.20.5
node3   Ready    <none>                 6m36s   v1.20.5

## 3. kubeadm install ##
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.dipterv1:6443 --token qc4fyo.f014z28sso9t8h2f \
    --discovery-token-ca-cert-hash sha256:ec4f76a12bc3d40f0d77e3f6841a843fbbe5229272fba96ea3ca1d08e7f92d4e \                                                                                                        
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join apiserver.dipterv1:6443 --token qc4fyo.f014z28sso9t8h2f \
    --discovery-token-ca-cert-hash sha256:ec4f76a12bc3d40f0d77e3f6841a843fbbe5229272fba96ea3ca1d08e7f92d4e 


kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml

## Number sok install ##

sudo kubeadm init --config=kubeadm-config-new.yaml | tee kubeadm-init.out.4
sudo iptables -F KUBE-FORWARD
sudo iptables -F KUBE-SERVICES
sudo iptables -F KUBE-EXTERNAL-SERVICES
sudo iptables -F KUBE-PROXY-CANARY
sudo iptables -F KUBE-KUBELET-CANARY
sudo iptables -F KUBE-FIREWALL


kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join apiserver.dipterv1:6443 --token qc4fyo.f014z28sso9t8h2f \
    --discovery-token-ca-cert-hash sha256:d69f50815debd4188e45e1cdca40927671c54aad0b51b0238e44339fa3c8bd58 \                                                                                                        
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join apiserver.dipterv1:6443 --token qc4fyo.f014z28sso9t8h2f \
    --discovery-token-ca-cert-hash sha256:d69f50815debd4188e45e1cdca40927671c54aad0b51b0238e44339fa3c8bd58    

NAMESPACE     NAME                            READY   STATUS    RESTARTS   AGE     IP             NODE    NOMINATED NODE   READINESS GATES
kube-system   coredns-74ff55c5b-czjzw         1/1     Running   0          5m2s    172.16.1.2     node2   <none>           <none>
kube-system   coredns-74ff55c5b-d7mnd         1/1     Running   0          5m2s    172.16.1.3     node2   <none>           <none>
kube-system   etcd-node1                      1/1     Running   0          5m7s    10.151.103.1   node1   <none>           <none>
kube-system   kube-apiserver-node1            1/1     Running   0          5m7s    10.151.103.1   node1   <none>           <none>
kube-system   kube-controller-manager-node1   1/1     Running   0          5m7s    10.151.103.1   node1   <none>           <none>
kube-system   kube-flannel-ds-85qll           1/1     Running   0          2m26s   10.151.103.1   node1   <none>           <none>
kube-system   kube-flannel-ds-cjxsp           1/1     Running   0          56s     10.151.103.3   node3   <none>           <none>
kube-system   kube-flannel-ds-vzrp4           1/1     Running   0          70s     10.151.103.2   node2   <none>           <none>
kube-system   kube-proxy-cltr2                1/1     Running   0          56s     10.151.103.3   node3   <none>           <none>
kube-system   kube-proxy-fkd9g                1/1     Running   0          70s     10.151.103.2   node2   <none>           <none>
kube-system   kube-proxy-pzmlk                1/1     Running   0          5m2s    10.151.103.1   node1   <none>           <none>
kube-system   kube-scheduler-node1            1/1     Running   0          5m7s    10.151.103.1   node1   <none>           <none>

## Futó VM, reinstall előtt ##
tutkovics@node1:~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: ens192: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:a8:42:77 brd ff:ff:ff:ff:ff:ff
    inet 10.151.103.1/16 brd 10.151.255.255 scope global dynamic ens192
       valid_lft 5601sec preferred_lft 5601sec
    inet6 fe80::250:56ff:fea8:4277/64 scope link 
       valid_lft forever preferred_lft forever
3: ens224: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 00:50:56:a8:2c:24 brd ff:ff:ff:ff:ff:ff
    inet 152.66.211.2/24 brd 152.66.211.255 scope global dynamic noprefixroute ens224
       valid_lft 6536sec preferred_lft 6536sec
    inet6 fe80::250:56ff:fea8:2c24/64 scope link noprefixroute 
       valid_lft forever preferred_lft forever
